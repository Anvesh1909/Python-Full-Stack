{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What is PySpark ? \n",
    "\n",
    "Apache Spark has become a popular framework for processing large-scale data and performing distributed computing tasks. With its powerful processing capabilities, PySpark, the Python API for Apache Spark, has gained significant traction among data engineers and data scientists.Spark keeps data in memory instead of disk, which allows it to access data much faster(10-100x faster.)\n",
    ".\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Description :\n",
    "\n",
    "Firsty , for using pyspark we have to install it.Therefore , we need to use pip install to install.The code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:33:50.800002Z",
     "iopub.status.busy": "2024-03-30T21:33:50.799564Z",
     "iopub.status.idle": "2024-03-30T21:34:07.481314Z",
     "shell.execute_reply": "2024-03-30T21:34:07.479758Z",
     "shell.execute_reply.started": "2024-03-30T21:33:50.799968Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\manve\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\manve\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Description :\n",
    "\n",
    "Now , easily we are able to import it , in addition we need to use of SparkSession which is an object that is the primary entry point for Spark applications, and allows you to run SQL queries on database tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:07.48479Z",
     "iopub.status.busy": "2024-03-30T21:34:07.484375Z",
     "iopub.status.idle": "2024-03-30T21:34:07.490416Z",
     "shell.execute_reply": "2024-03-30T21:34:07.488917Z",
     "shell.execute_reply.started": "2024-03-30T21:34:07.484751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Description :\n",
    "\n",
    "For loading a dataset and before that we need to create an app to be able to load our data , so that we use of the object that we have already imported and then\n",
    "for building there is a method with the name builder , then we need select a name for our app(anything it can be , but better if it is related to our work),the last use \n",
    "a method namely getOrCreate() to create our app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:07.493Z",
     "iopub.status.busy": "2024-03-30T21:34:07.492282Z",
     "iopub.status.idle": "2024-03-30T21:34:07.509752Z",
     "shell.execute_reply": "2024-03-30T21:34:07.508655Z",
     "shell.execute_reply.started": "2024-03-30T21:34:07.492959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "when we see , some information like version , Master , AppName will be shown ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:07.512801Z",
     "iopub.status.busy": "2024-03-30T21:34:07.512342Z",
     "iopub.status.idle": "2024-03-30T21:34:07.529747Z",
     "shell.execute_reply": "2024-03-30T21:34:07.528737Z",
     "shell.execute_reply.started": "2024-03-30T21:34:07.512719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Anvesh:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cd7ff57850>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we use a dataset to show the fundemantal functions in pyspark , but you are able to use any datase you want , however , you have to be careful that change the name of columns based on your dataset because all datasets are not the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:07.531584Z",
     "iopub.status.busy": "2024-03-30T21:34:07.531248Z",
     "iopub.status.idle": "2024-03-30T21:34:07.541842Z",
     "shell.execute_reply": "2024-03-30T21:34:07.540461Z",
     "shell.execute_reply.started": "2024-03-30T21:34:07.531548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# add = '/kaggle/input/house-price-tehran-iran/housePrice.csv'\n",
    "add = 'housePrice.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "we can easily use our spark and method read.csv() and after that the address of our dataset to be ok like panada method(pd.read_csv(add)), but with one difference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:07.544486Z",
     "iopub.status.busy": "2024-03-30T21:34:07.544005Z",
     "iopub.status.idle": "2024-03-30T21:34:07.816736Z",
     "shell.execute_reply": "2024-03-30T21:34:07.815479Z",
     "shell.execute_reply.started": "2024-03-30T21:34:07.544444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df = spark.read.csv('/kaggle/input/house-price-tehran-iran')\n",
    "df = spark.read.csv('housePrice.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "When we see our df we can see just a name of DataFrame and in a row _c0 till the number of columns will be shown , C stands for column and the number started from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:07.818541Z",
     "iopub.status.busy": "2024-03-30T21:34:07.818093Z",
     "iopub.status.idle": "2024-03-30T21:34:07.834244Z",
     "shell.execute_reply": "2024-03-30T21:34:07.832299Z",
     "shell.execute_reply.started": "2024-03-30T21:34:07.8185Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To solve this and see our dataset , we can use show() to see our dataset , however , the name of columns wont change but we are able to see by now ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:07.841164Z",
     "iopub.status.busy": "2024-03-30T21:34:07.840364Z",
     "iopub.status.idle": "2024-03-30T21:34:07.95805Z",
     "shell.execute_reply": "2024-03-30T21:34:07.955558Z",
     "shell.execute_reply.started": "2024-03-30T21:34:07.841115Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+---------+--------+--------------------+-------------+----------+\n",
      "| _c0| _c1|    _c2|      _c3|     _c4|                 _c5|          _c6|       _c7|\n",
      "+----+----+-------+---------+--------+--------------------+-------------+----------+\n",
      "|Area|Room|Parking|Warehouse|Elevator|             Address|        Price|Price(USD)|\n",
      "|  63|   1|   True|     True|    True|             Shahran| 1850000000.0|  61666.67|\n",
      "|  60|   1|   True|     True|    True|             Shahran| 1850000000.0|  61666.67|\n",
      "|  79|   2|   True|     True|    True|              Pardis|  550000000.0|  18333.33|\n",
      "|  95|   2|   True|     True|    True|       Shahrake Qods|  902500000.0|  30083.33|\n",
      "| 123|   2|   True|     True|    True|      Shahrake Gharb| 7000000000.0| 233333.33|\n",
      "|  70|   2|   True|     True|   False|North Program Org...| 2050000000.0|  68333.33|\n",
      "|  87|   2|   True|     True|    True|              Pardis|  600000000.0|   20000.0|\n",
      "|  59|   1|   True|     True|    True|             Shahran| 2150000000.0|  71666.67|\n",
      "|  54|   2|   True|     True|   False|            Andisheh|  493000000.0|  16433.33|\n",
      "|  71|   1|   True|     True|    True|West Ferdows Boul...| 2370000000.0|   79000.0|\n",
      "|  68|   2|   True|     True|    True|West Ferdows Boul...| 2450000000.0|  81666.67|\n",
      "|  64|   1|   True|     True|    True|              Narmak| 2100000000.0|   70000.0|\n",
      "|  54|   1|  False|     True|    True|              Narmak| 1690000000.0|  56333.33|\n",
      "| 136|   3|   True|     True|    True|         Saadat Abad|11000000000.0| 366666.67|\n",
      "|  95|   2|   True|     True|    True|               Zafar| 5000000000.0| 166666.67|\n",
      "|  63|   1|  False|     True|   False|          Islamshahr|  570000000.0|   19000.0|\n",
      "| 155|   3|   True|     True|    True|              Narmak| 6700000000.0| 223333.33|\n",
      "|  64|   2|  False|     True|   False|             Pirouzi| 1450000000.0|  48333.33|\n",
      "| 140|   3|   True|     True|    True|West Ferdows Boul...| 6400000000.0| 213333.33|\n",
      "+----+----+-------+---------+--------+--------------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To replace these odd names with the real names of our columns we can easily read our dataset another way , use read_option(  ,  ) in the method we need to determine two thngs , \n",
    "'header' , and the other is 'true' or 'false'.If it is true the name of our real dataset will be set ,otherwise the thing that we had before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:07.960788Z",
     "iopub.status.busy": "2024-03-30T21:34:07.95952Z",
     "iopub.status.idle": "2024-03-30T21:34:07.966305Z",
     "shell.execute_reply": "2024-03-30T21:34:07.965007Z",
     "shell.execute_reply.started": "2024-03-30T21:34:07.96074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  the thing that we had before  \n",
    "# sdf=spark.read.option('header' , 'false').csv(add)\n",
    "# sdf.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:07.973995Z",
     "iopub.status.busy": "2024-03-30T21:34:07.97304Z",
     "iopub.status.idle": "2024-03-30T21:34:08.31235Z",
     "shell.execute_reply": "2024-03-30T21:34:08.311057Z",
     "shell.execute_reply.started": "2024-03-30T21:34:07.973941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+---------+--------+--------------------+-------------+----------+\n",
      "|Area|Room|Parking|Warehouse|Elevator|             Address|        Price|Price(USD)|\n",
      "+----+----+-------+---------+--------+--------------------+-------------+----------+\n",
      "|  63|   1|   True|     True|    True|             Shahran| 1850000000.0|  61666.67|\n",
      "|  60|   1|   True|     True|    True|             Shahran| 1850000000.0|  61666.67|\n",
      "|  79|   2|   True|     True|    True|              Pardis|  550000000.0|  18333.33|\n",
      "|  95|   2|   True|     True|    True|       Shahrake Qods|  902500000.0|  30083.33|\n",
      "| 123|   2|   True|     True|    True|      Shahrake Gharb| 7000000000.0| 233333.33|\n",
      "|  70|   2|   True|     True|   False|North Program Org...| 2050000000.0|  68333.33|\n",
      "|  87|   2|   True|     True|    True|              Pardis|  600000000.0|   20000.0|\n",
      "|  59|   1|   True|     True|    True|             Shahran| 2150000000.0|  71666.67|\n",
      "|  54|   2|   True|     True|   False|            Andisheh|  493000000.0|  16433.33|\n",
      "|  71|   1|   True|     True|    True|West Ferdows Boul...| 2370000000.0|   79000.0|\n",
      "|  68|   2|   True|     True|    True|West Ferdows Boul...| 2450000000.0|  81666.67|\n",
      "|  64|   1|   True|     True|    True|              Narmak| 2100000000.0|   70000.0|\n",
      "|  54|   1|  False|     True|    True|              Narmak| 1690000000.0|  56333.33|\n",
      "| 136|   3|   True|     True|    True|         Saadat Abad|11000000000.0| 366666.67|\n",
      "|  95|   2|   True|     True|    True|               Zafar| 5000000000.0| 166666.67|\n",
      "|  63|   1|  False|     True|   False|          Islamshahr|  570000000.0|   19000.0|\n",
      "| 155|   3|   True|     True|    True|              Narmak| 6700000000.0| 223333.33|\n",
      "|  64|   2|  False|     True|   False|             Pirouzi| 1450000000.0|  48333.33|\n",
      "| 140|   3|   True|     True|    True|West Ferdows Boul...| 6400000000.0| 213333.33|\n",
      "|  42|   1|  False|     True|   False|             Pirouzi| 1390000000.0|  46333.33|\n",
      "+----+----+-------+---------+--------+--------------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is we we want instead that \n",
    "sdf=spark.read.option('header' , 'true').csv(add)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "we have to know a couple of preliminary functions for understanding our dataset. like pandas we have important functions which we have to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "head(num) function shows the head sample (one sample) and you can also write a number for instance instead  head(10) will show you the 10 head samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:08.316449Z",
     "iopub.status.busy": "2024-03-30T21:34:08.313666Z",
     "iopub.status.idle": "2024-03-30T21:34:08.462824Z",
     "shell.execute_reply": "2024-03-30T21:34:08.461493Z",
     "shell.execute_reply.started": "2024-03-30T21:34:08.316402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Also tail() function should you the tail samples of the dataset=> like the previous you can ask for the a couple of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:08.46461Z",
     "iopub.status.busy": "2024-03-30T21:34:08.464177Z",
     "iopub.status.idle": "2024-03-30T21:34:08.584363Z",
     "shell.execute_reply": "2024-03-30T21:34:08.583032Z",
     "shell.execute_reply.started": "2024-03-30T21:34:08.46457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "we can see the name of our columns by columns method.it returns a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:08.587144Z",
     "iopub.status.busy": "2024-03-30T21:34:08.586253Z",
     "iopub.status.idle": "2024-03-30T21:34:08.597635Z",
     "shell.execute_reply": "2024-03-30T21:34:08.596429Z",
     "shell.execute_reply.started": "2024-03-30T21:34:08.587076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Easily we can count the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:08.600363Z",
     "iopub.status.busy": "2024-03-30T21:34:08.599589Z",
     "iopub.status.idle": "2024-03-30T21:34:08.608247Z",
     "shell.execute_reply": "2024-03-30T21:34:08.607141Z",
     "shell.execute_reply.started": "2024-03-30T21:34:08.600316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(sdf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to know what types our columns are , we can use dtypes method to understand that.returns a list of our columns and its type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:08.610834Z",
     "iopub.status.busy": "2024-03-30T21:34:08.610009Z",
     "iopub.status.idle": "2024-03-30T21:34:08.62295Z",
     "shell.execute_reply": "2024-03-30T21:34:08.621858Z",
     "shell.execute_reply.started": "2024-03-30T21:34:08.610792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to know the number of samples we can use count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:08.626099Z",
     "iopub.status.busy": "2024-03-30T21:34:08.624981Z",
     "iopub.status.idle": "2024-03-30T21:34:08.770665Z",
     "shell.execute_reply": "2024-03-30T21:34:08.76955Z",
     "shell.execute_reply.started": "2024-03-30T21:34:08.626054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get some information about our dataset , we can use describe() function like Pandas but with a little difference.\n",
    "when we just use describe() we can see just columns with the type of each against the name , but in Pandas we get some more information like mean-std and so on...\n",
    "for that we have to use show() to illustrate that. let's check both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:08.775558Z",
     "iopub.status.busy": "2024-03-30T21:34:08.775089Z",
     "iopub.status.idle": "2024-03-30T21:34:08.866659Z",
     "shell.execute_reply": "2024-03-30T21:34:08.865193Z",
     "shell.execute_reply.started": "2024-03-30T21:34:08.775517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:08.869522Z",
     "iopub.status.busy": "2024-03-30T21:34:08.868025Z",
     "iopub.status.idle": "2024-03-30T21:34:09.63815Z",
     "shell.execute_reply": "2024-03-30T21:34:09.63664Z",
     "shell.execute_reply.started": "2024-03-30T21:34:08.869474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For checking the schemas we can easily use printSchema() to have that.You might not know what schema is , in that case : \n",
    "A Spark schema defines the structure of the data, specifying the column names and data.\n",
    "Let's understand that better... ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:09.640088Z",
     "iopub.status.busy": "2024-03-30T21:34:09.639596Z",
     "iopub.status.idle": "2024-03-30T21:34:09.653058Z",
     "shell.execute_reply": "2024-03-30T21:34:09.651537Z",
     "shell.execute_reply.started": "2024-03-30T21:34:09.64004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to select a specific column not all of them , just we need to use of select function to get that.From now on , remember that show() function is urgent to use , \n",
    "if dont use you get just type of the column not values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:09.656372Z",
     "iopub.status.busy": "2024-03-30T21:34:09.655774Z",
     "iopub.status.idle": "2024-03-30T21:34:09.685409Z",
     "shell.execute_reply": "2024-03-30T21:34:09.684214Z",
     "shell.execute_reply.started": "2024-03-30T21:34:09.656324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select('Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also write the number of Ages you want to be shown(if you dont write any number , whole will be shown but randomly 20 samples will show)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:09.687648Z",
     "iopub.status.busy": "2024-03-30T21:34:09.687206Z",
     "iopub.status.idle": "2024-03-30T21:34:09.792077Z",
     "shell.execute_reply": "2024-03-30T21:34:09.79077Z",
     "shell.execute_reply.started": "2024-03-30T21:34:09.687616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select('Area').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:09.79346Z",
     "iopub.status.busy": "2024-03-30T21:34:09.793141Z",
     "iopub.status.idle": "2024-03-30T21:34:09.882286Z",
     "shell.execute_reply": "2024-03-30T21:34:09.881073Z",
     "shell.execute_reply.started": "2024-03-30T21:34:09.793431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select('Area').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead show() you can use previous methods like head() , tail() for getting head and tail data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:09.884153Z",
     "iopub.status.busy": "2024-03-30T21:34:09.883646Z",
     "iopub.status.idle": "2024-03-30T21:34:09.973754Z",
     "shell.execute_reply": "2024-03-30T21:34:09.972589Z",
     "shell.execute_reply.started": "2024-03-30T21:34:09.884099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select('Area').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , if we want to get 2 columns together , we need to create a list of column names then like before use show , head , or tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:09.97543Z",
     "iopub.status.busy": "2024-03-30T21:34:09.975018Z",
     "iopub.status.idle": "2024-03-30T21:34:10.070051Z",
     "shell.execute_reply": "2024-03-30T21:34:10.068718Z",
     "shell.execute_reply.started": "2024-03-30T21:34:09.975393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select(['Area' , 'Parking']).tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to create a column which is related to another column we can use withcolumn(),for testing we are going to create a column that is the area of people's houses but plus 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:10.074199Z",
     "iopub.status.busy": "2024-03-30T21:34:10.073165Z",
     "iopub.status.idle": "2024-03-30T21:34:10.194117Z",
     "shell.execute_reply": "2024-03-30T21:34:10.192928Z",
     "shell.execute_reply.started": "2024-03-30T21:34:10.074154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('testcolumn' , sdf['Area'] + 1000)\n",
    "sdf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , if we want to drop any column we can use drop(name of that column) to remove the column.Let's remove the column that we have already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:10.196167Z",
     "iopub.status.busy": "2024-03-30T21:34:10.195664Z",
     "iopub.status.idle": "2024-03-30T21:34:10.358768Z",
     "shell.execute_reply": "2024-03-30T21:34:10.357588Z",
     "shell.execute_reply.started": "2024-03-30T21:34:10.196114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf = sdf.drop('testcolumn')\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For changing a column type , we can use cast() but we have to use withColumn to create a new one instead that.\n",
    "Here we change the Room column  and Price column to Boolean and float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:10.360917Z",
     "iopub.status.busy": "2024-03-30T21:34:10.360197Z",
     "iopub.status.idle": "2024-03-30T21:34:10.396732Z",
     "shell.execute_reply": "2024-03-30T21:34:10.395557Z",
     "shell.execute_reply.started": "2024-03-30T21:34:10.360873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"Parking\", sdf[\"Parking\"].cast('Boolean')) \n",
    "sdf = sdf.withColumn('Price' , sdf['Price'].cast('float'))\n",
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to remove null values we can use na() for finding the null values and then drop() for removing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:10.407641Z",
     "iopub.status.busy": "2024-03-30T21:34:10.406834Z",
     "iopub.status.idle": "2024-03-30T21:34:10.575457Z",
     "shell.execute_reply": "2024-03-30T21:34:10.574265Z",
     "shell.execute_reply.started": "2024-03-30T21:34:10.407592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.na.drop().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:10.577274Z",
     "iopub.status.busy": "2024-03-30T21:34:10.576766Z",
     "iopub.status.idle": "2024-03-30T21:34:10.58253Z",
     "shell.execute_reply": "2024-03-30T21:34:10.581428Z",
     "shell.execute_reply.started": "2024-03-30T21:34:10.57724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#sdf.na.drop(subset=['Column name']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:10.585321Z",
     "iopub.status.busy": "2024-03-30T21:34:10.584859Z",
     "iopub.status.idle": "2024-03-30T21:34:10.596784Z",
     "shell.execute_reply": "2024-03-30T21:34:10.595536Z",
     "shell.execute_reply.started": "2024-03-30T21:34:10.58528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fill Null values inside Address column with the word 'NaN' \n",
    "# df_pyspark.na.fill('NaN',subset=['Address']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are two ways that we can use for removing => all and any \n",
    "all means that all columns must be null in that case that sample will be removed \n",
    "any means even there is one null column for a a sample weill be removed\n",
    "to use these methods how is a paramater that we can difine as all or any in string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:10.599119Z",
     "iopub.status.busy": "2024-03-30T21:34:10.598595Z",
     "iopub.status.idle": "2024-03-30T21:34:10.724871Z",
     "shell.execute_reply": "2024-03-30T21:34:10.723959Z",
     "shell.execute_reply.started": "2024-03-30T21:34:10.599074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.na.drop(how='all').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:10.726133Z",
     "iopub.status.busy": "2024-03-30T21:34:10.725818Z",
     "iopub.status.idle": "2024-03-30T21:34:10.8623Z",
     "shell.execute_reply": "2024-03-30T21:34:10.86044Z",
     "shell.execute_reply.started": "2024-03-30T21:34:10.726106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.na.drop(how='any').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is another parameter which is momentous , which is thresh .\n",
    "The thresh parameter specifies the minimum number of non-null values that a row must have to be retained in the DataFrame.\n",
    "we can difine with a number 2 , 3 , 4 or whatever you want.for test here we put 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:10.86388Z",
     "iopub.status.busy": "2024-03-30T21:34:10.863507Z",
     "iopub.status.idle": "2024-03-30T21:34:10.989641Z",
     "shell.execute_reply": "2024-03-30T21:34:10.988377Z",
     "shell.execute_reply.started": "2024-03-30T21:34:10.863847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.na.drop(how='any' , thresh=2).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now , if we want to fill null values we can easily use fill('Missing') that replace that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:10.991368Z",
     "iopub.status.busy": "2024-03-30T21:34:10.990934Z",
     "iopub.status.idle": "2024-03-30T21:34:11.105532Z",
     "shell.execute_reply": "2024-03-30T21:34:11.104377Z",
     "shell.execute_reply.started": "2024-03-30T21:34:10.991327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#you can also put #NaN - nan - NAN -Null or any common names instead.\n",
    "sdf.na.fill('Missing').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to fill null values of a specific column we define the word we want to be replaced and then the name of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:11.107341Z",
     "iopub.status.busy": "2024-03-30T21:34:11.106887Z",
     "iopub.status.idle": "2024-03-30T21:34:11.228125Z",
     "shell.execute_reply": "2024-03-30T21:34:11.227174Z",
     "shell.execute_reply.started": "2024-03-30T21:34:11.107298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.na.fill('Missing' , 'Area').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however , we want do the same thing for more than one column but we have to put them in a bracket ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:11.229708Z",
     "iopub.status.busy": "2024-03-30T21:34:11.229277Z",
     "iopub.status.idle": "2024-03-30T21:34:11.363327Z",
     "shell.execute_reply": "2024-03-30T21:34:11.361219Z",
     "shell.execute_reply.started": "2024-03-30T21:34:11.22965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.na.fill('Missing' , ['Area' , 'Parking']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to find min , max , mean ,... we have to first import them and then we can use them ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:11.3651Z",
     "iopub.status.busy": "2024-03-30T21:34:11.36465Z",
     "iopub.status.idle": "2024-03-30T21:34:11.37276Z",
     "shell.execute_reply": "2024-03-30T21:34:11.370736Z",
     "shell.execute_reply.started": "2024-03-30T21:34:11.365049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:11.375309Z",
     "iopub.status.busy": "2024-03-30T21:34:11.374501Z",
     "iopub.status.idle": "2024-03-30T21:34:11.559076Z",
     "shell.execute_reply": "2024-03-30T21:34:11.55756Z",
     "shell.execute_reply.started": "2024-03-30T21:34:11.375266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select(min('Price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:11.560916Z",
     "iopub.status.busy": "2024-03-30T21:34:11.560356Z",
     "iopub.status.idle": "2024-03-30T21:34:11.728344Z",
     "shell.execute_reply": "2024-03-30T21:34:11.727184Z",
     "shell.execute_reply.started": "2024-03-30T21:34:11.560873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select(max('Price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:11.730318Z",
     "iopub.status.busy": "2024-03-30T21:34:11.729849Z",
     "iopub.status.idle": "2024-03-30T21:34:11.999615Z",
     "shell.execute_reply": "2024-03-30T21:34:11.998622Z",
     "shell.execute_reply.started": "2024-03-30T21:34:11.730273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select(mean('Price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:12.001256Z",
     "iopub.status.busy": "2024-03-30T21:34:12.000817Z",
     "iopub.status.idle": "2024-03-30T21:34:12.173357Z",
     "shell.execute_reply": "2024-03-30T21:34:12.172276Z",
     "shell.execute_reply.started": "2024-03-30T21:34:12.001216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select(std('Price')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to change the name of a column , we ca use withColumnrenamed(column name, new column name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:12.174784Z",
     "iopub.status.busy": "2024-03-30T21:34:12.174407Z",
     "iopub.status.idle": "2024-03-30T21:34:12.19669Z",
     "shell.execute_reply": "2024-03-30T21:34:12.195475Z",
     "shell.execute_reply.started": "2024-03-30T21:34:12.174751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.withColumnRenamed('Parking','My parking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to select a range of specific data , we are able to get data with collect()[index]\n",
    "it means that we can say exactly what data , or how many data we want to select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:12.199595Z",
     "iopub.status.busy": "2024-03-30T21:34:12.199091Z",
     "iopub.status.idle": "2024-03-30T21:34:12.407077Z",
     "shell.execute_reply": "2024-03-30T21:34:12.405285Z",
     "shell.execute_reply.started": "2024-03-30T21:34:12.199549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# we want to get the 0 index datum in the dataset , so we call 0 index in the bracket \n",
    "sdf.collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select data like the way we had in pandas , we can use [start , end , step] for collecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:12.411426Z",
     "iopub.status.busy": "2024-03-30T21:34:12.410077Z",
     "iopub.status.idle": "2024-03-30T21:34:12.563267Z",
     "shell.execute_reply": "2024-03-30T21:34:12.561599Z",
     "shell.execute_reply.started": "2024-03-30T21:34:12.411372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.collect()[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step means after the tenth datum you have to jump to the tewlevth datum and dont get 11th datum and so on ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:12.566056Z",
     "iopub.status.busy": "2024-03-30T21:34:12.564797Z",
     "iopub.status.idle": "2024-03-30T21:34:12.71119Z",
     "shell.execute_reply": "2024-03-30T21:34:12.709928Z",
     "shell.execute_reply.started": "2024-03-30T21:34:12.56601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.collect()[10 : 17 : 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can get data for a specific column like the previous one but with a little difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:12.7135Z",
     "iopub.status.busy": "2024-03-30T21:34:12.712788Z",
     "iopub.status.idle": "2024-03-30T21:34:12.990781Z",
     "shell.execute_reply": "2024-03-30T21:34:12.989431Z",
     "shell.execute_reply.started": "2024-03-30T21:34:12.713454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select('Area').collect()[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we can choose 2 columns or whatever , with collect and say which ones exactly we want ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:12.992874Z",
     "iopub.status.busy": "2024-03-30T21:34:12.992469Z",
     "iopub.status.idle": "2024-03-30T21:34:13.122023Z",
     "shell.execute_reply": "2024-03-30T21:34:13.121074Z",
     "shell.execute_reply.started": "2024-03-30T21:34:12.992842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.select(['Parking' , 'Price']).collect()[10: 20 : 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use filter() to get some specific data or filter a range of data like outliers or anything else . for instance , I filter parkings = 0 and price more than a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:13.12861Z",
     "iopub.status.busy": "2024-03-30T21:34:13.126366Z",
     "iopub.status.idle": "2024-03-30T21:34:13.247257Z",
     "shell.execute_reply": "2024-03-30T21:34:13.245979Z",
     "shell.execute_reply.started": "2024-03-30T21:34:13.128566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.filter(sdf.Parking == 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:13.251244Z",
     "iopub.status.busy": "2024-03-30T21:34:13.248524Z",
     "iopub.status.idle": "2024-03-30T21:34:13.399017Z",
     "shell.execute_reply": "2024-03-30T21:34:13.397794Z",
     "shell.execute_reply.started": "2024-03-30T21:34:13.251192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sdf.filter(sdf.Price > 60000000000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the sample function in PySpark to select a random sample of rows from a DataFrame.\n",
    "\n",
    "This function uses the following syntax:\n",
    "\n",
    "sample(withReplacement=None, fraction=None, seed=None)\n",
    "\n",
    "where:\n",
    "\n",
    "withReplacement: Whether to sample with replacement or not (default=False)\n",
    "fraction: Fraction of rows to include in sample\n",
    "seed: An integer that specifies the random seed for sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    It shows 20% of whole teh data randomly.when withReplacement is False means no need for replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:13.400943Z",
     "iopub.status.busy": "2024-03-30T21:34:13.4003Z",
     "iopub.status.idle": "2024-03-30T21:34:13.571154Z",
     "shell.execute_reply": "2024-03-30T21:34:13.569893Z",
     "shell.execute_reply.started": "2024-03-30T21:34:13.400893Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Randomselection=sdf.sample(withReplacement=False , fraction=0.2 , seed=None)\n",
    "Randomselection.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to save our dataset(the new with changes) we can use write.saveAsTable(address)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:13.573201Z",
     "iopub.status.busy": "2024-03-30T21:34:13.572668Z",
     "iopub.status.idle": "2024-03-30T21:34:13.579976Z",
     "shell.execute_reply": "2024-03-30T21:34:13.578197Z",
     "shell.execute_reply.started": "2024-03-30T21:34:13.573154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sdf.write.saveAsTable(\"us_cities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case your file is JSON and you want to save it , this line of code will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:13.583999Z",
     "iopub.status.busy": "2024-03-30T21:34:13.582867Z",
     "iopub.status.idle": "2024-03-30T21:34:13.593996Z",
     "shell.execute_reply": "2024-03-30T21:34:13.592478Z",
     "shell.execute_reply.started": "2024-03-30T21:34:13.583937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df.write.format(\"json\").save(\"/tmp/json_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a JSON file you should use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T21:34:13.597859Z",
     "iopub.status.busy": "2024-03-30T21:34:13.596607Z",
     "iopub.status.idle": "2024-03-30T21:34:13.605634Z",
     "shell.execute_reply": "2024-03-30T21:34:13.60409Z",
     "shell.execute_reply.started": "2024-03-30T21:34:13.597809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df3 = spark.read.format(\"json\").json(\"/tmp/json_data\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1573831,
     "sourceId": 2590700,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "b55d1d67820a95cf2b431bcf457a948aa732b20fc43a943255adf501a31a5072"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
